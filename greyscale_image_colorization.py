# -*- coding: utf-8 -*-
"""GrayScale_Image_Colorization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UOQTlTWkKdn3HJ_EQyFYYURCR7TnNONV

**Summary and Overview**

This model, at its core, leverages ***torch*** for computer vision application.

The model uses Microsoft Common Objects in Context (COCO) dataset, the gold standard benchmark for evaluating the performance of state of the art computer vision models.

The key steps are:

**Library Installation & Imports:**

Install necessary libraries and import required modules for building the image colorization model.

**Device Configuration:**

Code to set the device for computations - Metal Performance Shaders (MPS), CUDA, and/or CPU, and printing the selected device.

**Data Handling:**

Mounting Google Drive to store and to facilitate access to datasets stored there.

**Dataset Download and Preparation:**

Set up the directory for the COCO dataset, downloading test images and unzipping them.

**Image Path Loading:**

Load image paths for training and validation from the COCO dataset. Select samples and split them into training and validation sets.

**Dataset Class Definition:**

Dataset: Microsoft Common Objects in Context (COCO) dataset, test_2017 dataset.

Customize Image Colorization Data class from the Dataset, initialize with image paths and size, load and process images, and provide the length of the dataset.

**DataLoader Creation:**

Create DataLoader instances for both training and validation datasets with specified batch sizes and shuffling options.

**Generator Model Definition:**

Build ResNet-based UNet generator model for colorization.
"""

# Install necessary libraries, use the pre-built wheels for TPU from PyTorch directly
!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 torch-xla

!pip install torch==2.0.0 torchvision==0.15.1 torch-xla==2.0

# Import necessary libraries
import os
import gc
import numpy as np
import pickle
import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torchvision.models.resnet import resnet18
from fastai.vision.learner import create_body
from fastai.vision.models.unet import DynamicUnet
from PIL import Image
from skimage.color import rgb2lab, lab2rgb
import matplotlib.pyplot as plt
import warnings

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning, module="skimage._shared.utils")

# Device setup
device = torch.device("mps" if torch.has_mps else "cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Mount Google Drive for data handling
from google.colab import drive

try:
    drive.mount('/content/drive')
except Exception as e:
    print("Error while mounting Google Drive:", str(e))

# Data directory and downloading COCO dataset
wdir_data = "./coco_test2017"
os.makedirs(wdir_data, exist_ok=True)

if not os.path.exists(os.path.join(wdir_data, 'test2017')):
    !wget http://images.cocodataset.org/zips/test2017.zip -O coco_test2017.zip
    !unzip coco_test2017.zip -d coco_test2017
    !rm coco_test2017.zip

# Clear cache
gc.collect()

# Load image paths
coco_image_dir = './coco_test2017/test2017'
paths = [os.path.join(coco_image_dir, f) for f in os.listdir(coco_image_dir) if f.endswith('.jpg')]

# Randomly select and shuffle the dataset
np.random.seed(42)
paths_subset = np.random.choice(paths, 10000, replace=False)
rand_idxs = np.random.permutation(10000)
train_idxs = rand_idxs[:8000]
val_idxs = rand_idxs[8000:]
train_paths = paths_subset[train_idxs]
val_paths = paths_subset[val_idxs]

print(f"Training paths: {len(train_paths)}, Validation paths: {len(val_paths)}")

# Save the selected train and validation paths
paths_save_file = 'selected_image_paths.pkl'
with open(paths_save_file, 'wb') as f:
    pickle.dump({'train_paths': train_paths, 'val_paths': val_paths}, f)
print(f"Saved train and validation paths to {paths_save_file}")

# Image Colorization Dataset
class ImageColorizationData(Dataset):
    """Custom Dataset for loading images and converting them to LAB color space."""

    def __init__(self, paths, size):
        """
        Initializes the dataset with provided image paths and target size.

        Args:
            paths (list): List of image file paths.
            size (int): Target size to resize images to.
        """
        self.transforms = transforms.Resize((size, size), Image.BICUBIC)
        self.size = size
        self.paths = paths

    def __getitem__(self, idx):
        """Loads and processes an image at the given index."""
        img = Image.open(self.paths[idx]).convert("RGB")  # Convert to RGB
        img = self.transforms(img)
        img = np.array(img)  # Convert to NumPy array
        img_lab = rgb2lab(img).astype("float32")  # Convert RGB to LAB color space
        img_lab = transforms.ToTensor()(img_lab)  # Convert to tensor
        L = img_lab[[0], ...] / 50. - 1.  # Normalize L channel to [-1, 1]
        ab = img_lab[[1, 2], ...] / 110.  # Normalize ab channels to [-1, 1]
        return L, ab

    def __len__(self):
        """Returns the total number of images in the dataset."""
        return len(self.paths)

# Load the selected image paths: load from these paths at any point later in the program,
# to work with the pre-saved paths instead of re-running the selection process
with open('selected_image_paths.pkl', 'rb') as f:
    loaded_paths = pickle.load(f)
    train_paths = loaded_paths['train_paths']
    val_paths = loaded_paths['val_paths']

print(f"Loaded train paths: {len(train_paths)}, Loaded validation paths: {len(val_paths)}")

# Build DataLoader
size = 256
train_data = ImageColorizationData(train_paths, size)
test_data = ImageColorizationData(val_paths, size)

train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=8, shuffle=False)

# Build the ResNet UNet model
def build_res_unet(n_input=1, n_output=2, size=256):
    body = create_body(resnet18(pretrained=True), n_in=n_input, cut=-2)
    pretrained_Unet = DynamicUnet(body, n_output, (size, size))
    return pretrained_Unet

# Define the Discriminator
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)
        )

    def forward(self, x):
        return self.model(x)

# Custom GAN Loss
class GANLoss(nn.Module):
    def __init__(self):
        super(GANLoss, self).__init__()
        self.real_label = 1.0
        self.fake_label = 0.0
        self.loss = nn.BCEWithLogitsLoss()

    def get_labels(self, preds, target_is_real):
        return torch.full_like(preds, fill_value=self.real_label if target_is_real else self.fake_label)

    def forward(self, preds, target_is_real):
        labels = self.get_labels(preds, target_is_real)
        loss = self.loss(preds, labels)
        return loss

# Weight Initialization
def init_weights(model, init='norm', gain=0.02):
    def init_func(m):
        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
            if init == 'norm':
                nn.init.normal_(m.weight.data, mean=0.0, std=gain)
            elif init == 'xavier':
                nn.init.xavier_normal_(m.weight.data, gain=gain)
            elif init == 'kaiming':
                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
            if m.bias is not None:
                nn.init.constant_(m.bias.data, 0.0)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.normal_(m.weight.data, 1., gain)
            nn.init.constant_(m.bias.data, 0.)

    model.apply(init_func)
    print(f"Model initialized with {init} initialization")
    return model

# Colorization GAN Model
class ColorizationResnetGan(nn.Module):
    def __init__(self):
        super(ColorizationResnetGan, self).__init__()
        self.generator_net = build_res_unet().to(device)
        self.discriminator_net = init_weights(Discriminator()).to(device)

    def forward(self, L):
        fake_color = self.generator_net(L)
        return fake_color

# Convert LAB to RGB
def lab_to_rgb(L, ab):
    L = (L + 1.) * 50.
    ab = ab * 110.
    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()
    rgb_imgs = [lab2rgb(img) for img in Lab]
    return np.stack(rgb_imgs, axis=0)

# Initialize the model
model = ColorizationResnetGan()

# Define optimizers
optimizer_G = optim.Adam(model.generator_net.parameters(), lr=2e-4, betas=(0.5, 0.999))
optimizer_D = optim.Adam(model.discriminator_net.parameters(), lr=2e-4, betas=(0.5, 0.999))

# Loss functions
criterion_GAN = GANLoss().to(device)
criterion_L1 = nn.L1Loss()

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    model.generator_net.train()
    model.discriminator_net.train()

    # Track losses
    total_discrim_loss = 0
    total_gener_loss = 0
    total_gener_l1_loss = 0

    for batch_idx, (L, ab) in enumerate(train_dataloader):
        L, ab = L.to(device), ab.to(device)

        # Train Discriminator
        optimizer_D.zero_grad()
        fake_color = model.generator_net(L).detach()
        fake_images = torch.cat([L, fake_color], dim=1)
        preds_fake = model.discriminator_net(fake_images)
        loss_discrim_fake = criterion_GAN(preds_fake, False)

        true_images = torch.cat([L, ab], dim=1)
        preds_true = model.discriminator_net(true_images)
        loss_discrim_true = criterion_GAN(preds_true, True)

        loss_discrim_total = (loss_discrim_fake + loss_discrim_true) * 0.5
        loss_discrim_total.backward()

        # Update the discriminator's parameters
        optimizer_D.step()
        total_discrim_loss += loss_discrim_total.item()

        # Train Generator
        optimizer_G.zero_grad()
        fake_color = model.generator_net(L)  # Forward pass
        fake_images = torch.cat([L, fake_color], dim=1)
        preds_fake = model.discriminator_net(fake_images)

        # Calculate generator losses
        loss_gener_gan = criterion_GAN(preds_fake, True)
        loss_gener_l1 = criterion_L1(fake_color, ab) * 100.  # Scaling L1 loss
        loss_gener_total = loss_gener_gan + loss_gener_l1
        loss_gener_total.backward()
        optimizer_G.step()

        total_gener_loss += loss_gener_total.item()
        total_gener_l1_loss += loss_gener_l1.item() / 100.

        # Clear cache if necessary (avoiding memory overflow)
        del fake_color, fake_images, preds_fake, loss_discrim_fake, preds_true
        del loss_discrim_true, loss_discrim_total, loss_gener_gan, loss_gener_l1, loss_gener_total

    # Average losses over batches
    avg_discrim_loss = total_discrim_loss / len(train_dataloader)
    avg_gener_loss = total_gener_loss / len(train_dataloader)
    avg_gener_l1_loss = total_gener_l1_loss / len(train_dataloader)

    print(f"Epoch {epoch + 1}/{num_epochs}")
    print(f"Discriminator loss: {avg_discrim_loss:.4f}")
    print(f"Generator loss: {avg_gener_loss:.4f}")
    print(f"Generator L1 loss: {avg_gener_l1_loss:.4f}")
    print("-" * 10)

    # Evaluation on validation set every 5 epochs
    if (epoch + 1) % 5 == 0:
        model.generator_net.eval()
        overall_test_loss = 0
        with torch.no_grad():
            for batch_idx, (L_test, ab_true_test) in enumerate(test_dataloader):
                L_test, ab_true_test = L_test.to(device), ab_true_test.to(device)
                ab_pred_test = model.generator_net(L_test)
                test_loss = criterion_L1(ab_pred_test, ab_true_test)
                overall_test_loss += test_loss.item()

        avg_test_loss = overall_test_loss / len(test_dataloader)
        print(f"Test loss: {avg_test_loss:.4f}")

        # Sample Predictions
        data = next(iter(test_dataloader))
        L_test, ab_true_test = data
        L_test, ab_true_test = L_test.to(device), ab_true_test.to(device)
        ab_pred_test = model.generator_net(L_test)

        # Detach tensors before passing to lab_to_rgb
        corrected_pred = lab_to_rgb(L_test.detach(), ab_pred_test.detach())
        corrected_orig = lab_to_rgb(L_test.detach(), ab_true_test.detach())

        for i in range(len(corrected_orig)):
            plt.figure(figsize=(10, 5))
            plt.subplot(1, 2, 1)
            plt.imshow(corrected_orig[i])
            plt.title('Ground Truth (RGB)')
            plt.subplot(1, 2, 2)
            plt.imshow(corrected_pred[i])
            plt.title('Prediction (RGB)')
            plt.show()

    # Save the model every 2 epochs
    if (epoch + 1) % 2 == 0:
        torch.save(model.generator_net.state_dict(), "./colorization_gan_model.pth")
        torch.save(model.discriminator_net.state_dict(), "./discriminator_gan_model.pth")

print("Training Complete!")

# After training, visualize some results
model.eval()
with torch.no_grad():
    # Get a batch of test data
    L, ab = next(iter(test_dataloader))
    L = L.to(device)
    ab = ab.to(device)

    # Generate predicted ab channels from the generator network
    ab_pred = model.generator_net(L)

    # Convert LAB predictions and original images to RGB
    pred_img = lab_to_rgb(L, ab_pred)
    orig_img = lab_to_rgb(L, ab)

    # Show some sample images
    for idx, img in enumerate(pred_img):
        plt.figure(figsize=(15, 5))
        # Display Ground Truth
        plt.subplot(1, 3, 1)
        plt.imshow(orig_img[idx])
        plt.title('Ground Truth (RGB)')
        plt.axis('off')  # Hide axes for better visualization

        # Display Greyscale version (L channel only)
        plt.subplot(1, 3, 2)
        plt.imshow(lab_to_rgb(L[idx], torch.zeros_like(ab[idx]))[0])
        plt.title('Greyscale (RGB)')
        plt.axis('off')  # Hide axes for better visualization

        # Display the Generated Prediction
        plt.subplot(1, 3, 3)
        plt.imshow(img)
        plt.title('Prediction (RGB)')
        plt.axis('off')  # Hide axes for better visualization

        plt.show()  # Show the figures

# Save model to CPU for later use
model.cpu()
torch.save(model.generator_net.state_dict(), "./colorization_gan_model_cpu.pth")
torch.save(model.discriminator_net.state_dict(), "./discriminator_gan_model_cpu.pth")

# Save model to GPU for later use
model.cuda()
torch.save(model.generator_net.state_dict(), "./colorization_gan_model.pth")
torch.save(model.discriminator_net.state_dict(), "./discriminator_gan_model.pth")

import numpy as np
import matplotlib.pyplot as plt
from skimage.color import lab2rgb

# Updated lab_to_rgb function
def lab_to_rgb(L, ab):
    # Reverse the scaling of L and ab into LAB color space
    L = (L + 1.) * 50.  # Scale from [-1, 1] back to [0, 100]
    ab = ab * 110.      # Scale from [-1, 1] back to [0, 110]

    # Check dimensions
    if L.dim() == 3:  # If L has 3 dimensions, add a channel dimension
        L = L.unsqueeze(1)  # Convert to (B, 1, H, W)

    if ab.dim() == 3:  # If ab has 3 dimensions, add a channel dimension
        ab = ab.unsqueeze(0)  # Convert to (1, 2, H, W)

    # Concatenate L and ab channels and permute dimensions
    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()

    # Convert each image in the batch from Lab to RGB
    rgb_imgs = [lab2rgb(img) for img in Lab]
    return np.stack(rgb_imgs, axis=0)


# Visualize some results
model.eval()
with torch.no_grad():
    # Get a batch of test data
    L, ab = next(iter(test_dataloader))
    L = L.to(device)
    ab = ab.to(device)

    # Generate predicted ab channels from the generator network
    ab_pred = model.generator_net(L)

    # Convert LAB predictions and original images to RGB
    pred_img = lab_to_rgb(L, ab_pred)
    orig_img = lab_to_rgb(L, ab)

    # Show some sample images
    for idx, img in enumerate(pred_img):
        plt.figure(figsize=(15, 5))
        # Display Ground Truth
        plt.subplot(1, 3, 1)
        plt.imshow(orig_img[idx])
        plt.title('Ground Truth (RGB)')
        plt.axis('off')  # Hide axes for better visualization

        # Display Greyscale version (L channel only)
        plt.subplot(1, 3, 2)
        grey_image = lab_to_rgb(L[idx].unsqueeze(0), torch.zeros_like(ab[idx].unsqueeze(0)))  # Reshape L
        plt.imshow(grey_image[0])  # Extract the single image from batch
        plt.title('Greyscale (RGB)')
        plt.axis('off')  # Hide axes for better visualization

        # Display the Generated Prediction
        plt.subplot(1, 3, 3)
        plt.imshow(img)
        plt.title('Prediction (RGB)')
        plt.axis('off')  # Hide axes for better visualization

        plt.show()  # Show the figures

# Save model to CPU for later use
model.cpu()
torch.save(model.generator_net.state_dict(), "./colorization_gan_model_cpu.pth")
torch.save(model.discriminator_net.state_dict(), "./discriminator_gan_model_cpu.pth")

# Save model to GPU for later use
model.cuda()
torch.save(model.generator_net.state_dict(), "./colorization_gan_model.pth")
torch.save(model.discriminator_net.state_dict(), "./discriminator_gan_model.pth")